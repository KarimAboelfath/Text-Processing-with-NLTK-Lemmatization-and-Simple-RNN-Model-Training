{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558f16b9",
   "metadata": {},
   "source": [
    "# Text Processing with NLTK: Lemmatization and Simple RNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17fc42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Karim\n",
      "[nltk_data]     Nasr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Karim\n",
      "[nltk_data]     Nasr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')  # Download tokenizer if not already installed\n",
    "nltk.download('stopwords')  # Download stopwords if not already installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5d4cb",
   "metadata": {},
   "source": [
    "### Text Cleaning and Lemmatization\n",
    "\n",
    "This function `clean_text_Lemmatization` is designed to clean text data by performing several steps:\n",
    "\n",
    "1. **Lowercase Conversion:** The function converts all text to lowercase to ensure uniformity.\n",
    "2. **URL Removal:** It removes any URLs present in the text using a regular expression pattern.\n",
    "3. **Special Character Removal:** All special characters except hyphens and apostrophes are removed to avoid noise in the text.\n",
    "4. **Punctuation Removal:** Punctuation marks are removed from the text.\n",
    "5. **Stopword Removal (Optional):** If enabled, common stopwords in English are removed from the text.\n",
    "6. **Tokenization:** The text is tokenized into individual words.\n",
    "7. **Lemmatization:** Using WordNet lemmatizer from the NLTK library, words are lemmatized to their base form.\n",
    "\n",
    "**Arguments:**\n",
    "- `text (str)`: The text data to be cleaned.\n",
    "\n",
    "**Returns:**\n",
    "- `list`: A list of cleaned tokens ready for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d3c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_Lemmatization(text):\n",
    "    \"\"\"\n",
    "    Cleans text by performing the following steps:\n",
    "\n",
    "    1. Lowercase conversion\n",
    "    2. URL removal\n",
    "    3. Special character removal (excluding hyphen and apostrophe)\n",
    "    4. Punctuation removal\n",
    "    5. Stop word removal (customizable option)\n",
    "    6. Tokenization\n",
    "    7. Lemmatization\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Lowercase conversion\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. URL removal\n",
    "    url_pattern = r\"http[s]?://\\S+\\b|\\bwww\\.\\S+\\b\"  # Improved URL pattern\n",
    "    text = re.sub(url_pattern, \"\", text)\n",
    "\n",
    "    # 3. Special character removal (excluding hyphen and apostrophe)\n",
    "    special_char_pattern = r\"[^a-zA-Z0-9\\-\\']\"\n",
    "    text = re.sub(special_char_pattern, \" \", text)\n",
    "\n",
    "    # 4. Punctuation removal\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "\n",
    "    # 5. Stop word removal (optional)\n",
    "    remove_stopwords = True  # Flag to enable/disable stop word removal\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # 6. Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # 7. Stemming (using PorterStemmer for clarity)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ed9c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wind', 'whispered', 'tall', 'grass', 'carrying', 'scent', 'rain', 'distant', 'adventure', 'sarah', 'stood', 'edge', 'cliff', 'hair', 'billowing', 'around', 'like', 'cloak', 'gazed', 'endless', 'expanse', 'ocean', 'wave', 'crashed', 'rock', 'rhythmic', 'symphony', 'soothing', 'balm', 'troubled', 'mind', 'always', 'drawn', 'sea', 'vastness', 'mirroring', 'depth', 'soul', 'sense', 'freedom', 'liberation', 'constraint', 'everyday', 'life', 'crashing', 'wave', 'felt', 'worry', 'wash', 'away', 'replaced', 'sense', 'peace', 'clarity', 'today', 'something', 'different', 'air', 'something', 'electric', 'charged', 'watched', 'storm', 'began', 'brew', 'horizon', 'dark', 'cloud', 'swirling', 'ominously', 'overhead', 'yet', 'instead', 'fear', 'sarah', 'felt', 'surge', 'excitement', 'coursing', 'vein', 'long', 'could', 'remember', 'yearned', 'adventure', 'life', 'le', 'ordinary', 'seemed', 'fate', 'finally', 'calling', 'name', 'fierce', 'determination', 'burning', 'heart', 'took', 'step', 'forward', 'towards', 'unknown', 'journey', 'ahead', 'would', 'fraught', 'peril', 'uncertainty', 'sarah', 'welcomed', 'open', 'arm', 'knew', 'venturing', 'unknown', 'could', 'truly', 'discover', 'meant', 'wind', 'back', 'ocean', 'foot', 'sarah', 'set', 'sail', 'storm', 'ready', 'embrace', 'whatever', 'lay', 'ahead', 'moment', 'knew', 'destiny', 'awaited', 'side', 'horizon']\n"
     ]
    }
   ],
   "source": [
    "# Define the text\n",
    "text = \"\"\"\n",
    "\"The wind whispered through the tall grass, carrying with it the scent of rain and distant adventures. Sarah stood at the edge of the cliff, her hair billowing around her like a cloak as she gazed out at the endless expanse of the ocean. The waves crashed against the rocks below, their rhythmic symphony a soothing balm to her troubled mind.\n",
    "\n",
    "She had always been drawn to the sea, its vastness mirroring the depths of her own soul. There was a sense of freedom here, a liberation from the constraints of everyday life. With each crashing wave, she felt her worries wash away, replaced by a sense of peace and clarity.\n",
    "\n",
    "But today, there was something different in the air, something electric and charged. As she watched, a storm began to brew on the horizon, dark clouds swirling ominously overhead. Yet, instead of fear, Sarah felt a surge of excitement coursing through her veins.\n",
    "\n",
    "For as long as she could remember, she had yearned for adventure, for a life less ordinary. And now, it seemed, fate was finally calling her name. With a fierce determination burning in her heart, she took a step forward, towards the unknown.\n",
    "\n",
    "The journey ahead would be fraught with peril and uncertainty, but Sarah welcomed it with open arms. For she knew that only by venturing into the unknown could she truly discover who she was meant to be.\n",
    "\n",
    "And so, with the wind at her back and the ocean at her feet, Sarah set sail into the storm, ready to embrace whatever lay ahead. For in that moment, she knew that her destiny awaited her on the other side of the horizon.\"\n",
    "\"\"\"\n",
    "\n",
    "cleaned_tokens_lemm = clean_text_Lemmatization(text)\n",
    "print(cleaned_tokens_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8292559",
   "metadata": {},
   "source": [
    "### Tokenizing Text and Creating Training Data\n",
    "\n",
    "This code cell utilizes the `clean_text_Lemmatization` function to clean the text and then tokenize it. Additionally, it creates training data for a word prediction model.\n",
    "\n",
    "- **Tokenization:** The cleaned text is tokenized to form individual words.\n",
    "- **Vocabulary Generation:** A set of unique words is created from the tokenized text.\n",
    "- **Mapping Words to Indices:** Each word in the vocabulary is mapped to a unique index for numerical representation.\n",
    "- **Creating Training Samples:** The text is then divided into context-target pairs for training the word prediction model.\n",
    "\n",
    "**Libraries Used:**\n",
    "- `torch`: PyTorch library for building neural networks.\n",
    "- `numpy`: NumPy library for numerical operations.\n",
    "\n",
    "**Returns:**\n",
    "- `data`: A list of tuples containing context-target pairs for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e77f482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 844 ms\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the text\n",
    "words = clean_text_Lemmatization(text)\n",
    "vocab = set(words)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(0, len(words) - 2):\n",
    "    context = [word_to_ix[words[i]], word_to_ix[words[i + 1]]]\n",
    "    target = word_to_ix[words[i + 2]]\n",
    "    data.append((context, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caadcae8",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN) Model Definition\n",
    "\n",
    "This code cell defines an RNN model for word prediction. The model architecture consists of the following components:\n",
    "\n",
    "- **Embedding Layer:** An embedding layer that maps each word index to a dense vector representation.\n",
    "- **RNN Layer:** An RNN layer that takes the embedded inputs and produces hidden states.\n",
    "- **Linear Layer:** A linear layer that predicts the next word in the sequence based on the final hidden state.\n",
    "\n",
    "**Arguments:**\n",
    "- `vocab_size`: The size of the vocabulary.\n",
    "- `embedding_dim`: The dimensionality of the word embeddings.\n",
    "- `hidden_dim`: The dimensionality of the hidden state of the RNN.\n",
    "\n",
    "**Methods:**\n",
    "- `forward`: Defines the forward pass of the model, taking inputs and returning the predicted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97b1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        rnn_out, _ = self.rnn(embeds.view(len(inputs), 1, -1))\n",
    "        output = self.linear(rnn_out.view(len(inputs), -1))\n",
    "        return output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4183b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f663d5",
   "metadata": {},
   "source": [
    "### Model Training Setup\n",
    "\n",
    "This code cell sets up the model training process by defining hyperparameters, initializing the model, loss function, and optimizer, and splitting the data into training and validation sets.\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `EMBEDDING_DIM`: Dimensionality of the word embeddings.\n",
    "- `HIDDEN_DIM`: Dimensionality of the hidden state of the RNN.\n",
    "- `LEARNING_RATE`: Learning rate for the optimizer.\n",
    "- `EPOCHS`: Number of training epochs.\n",
    "\n",
    "**Model Initialization:**\n",
    "- An instance of the `RNN` class is initialized with the specified hyperparameters.\n",
    "\n",
    "**Loss Function:**\n",
    "- Cross-Entropy Loss is used as the loss function for training.\n",
    "\n",
    "**Optimizer:**\n",
    "- Stochastic Gradient Descent (SGD) optimizer is employed for parameter optimization.\n",
    "\n",
    "**Data Splitting:**\n",
    "- The data is split into training and validation sets using an 80-20 split ratio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6499cae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 901 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_DIM = 10\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 100\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = RNN(len(vocab), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets (80% - 20%)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b0ab0",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "This code cell contains the training loop where the model is trained over multiple epochs using the training data. After each epoch, the training loss is computed and printed. Additionally, the validation loss is calculated to monitor the model's performance on unseen data.\n",
    "\n",
    "**Training Process:**\n",
    "- For each epoch, the total loss is initialized.\n",
    "- For each training sample in the training data:\n",
    "  - The model gradients are zeroed.\n",
    "  - The context and target indices are converted to PyTorch tensors.\n",
    "  - The model predicts the log probabilities of the next word.\n",
    "  - The loss is calculated using the predicted probabilities and the actual target.\n",
    "  - Gradients are backpropagated through the network.\n",
    "  - The optimizer updates the model parameters.\n",
    "- After each epoch, the training loss is computed and printed.\n",
    "\n",
    "**Validation Process:**\n",
    "- The model is switched to evaluation mode.\n",
    "- The total validation loss is initialized.\n",
    "- For each validation sample in the validation data:\n",
    "  - The context and target indices are converted to PyTorch tensors.\n",
    "  - The model predicts the log probabilities of the next word.\n",
    "  - The validation loss is calculated using the predicted probabilities and the actual target.\n",
    "- After each epoch, the validation loss is computed and printed.\n",
    "\n",
    "**Note:** The model is switched back to training mode after validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2c2a9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Training Loss: 1.6664 Validation Loss: 6.2019\n",
      "Epoch 20/100, Training Loss: 0.5274 Validation Loss: 7.5834\n",
      "Epoch 30/100, Training Loss: 0.2545 Validation Loss: 8.3788\n",
      "Epoch 40/100, Training Loss: 0.1577 Validation Loss: 8.9227\n",
      "Epoch 50/100, Training Loss: 0.1139 Validation Loss: 9.3189\n",
      "Epoch 60/100, Training Loss: 0.0891 Validation Loss: 9.6255\n",
      "Epoch 70/100, Training Loss: 0.0732 Validation Loss: 9.8758\n",
      "Epoch 80/100, Training Loss: 0.0622 Validation Loss: 10.0883\n",
      "Epoch 90/100, Training Loss: 0.0540 Validation Loss: 10.2732\n",
      "Epoch 100/100, Training Loss: 0.0478 Validation Loss: 10.4370\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for context, target in train_data:\n",
    "        model.zero_grad()\n",
    "\n",
    "        context_idxs = torch.tensor(context, dtype=torch.long)\n",
    "        target = torch.tensor([target], dtype=torch.long)\n",
    "\n",
    "        log_probs = model(context_idxs)\n",
    "        loss = loss_function(log_probs.unsqueeze(0), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS}, Training Loss: {total_loss/len(train_data):.4f}', end=' ')\n",
    "\n",
    "    # Compute validation loss\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for context, target in val_data:\n",
    "            context_idxs = torch.tensor(context, dtype=torch.long)\n",
    "            target = torch.tensor([target], dtype=torch.long)\n",
    "\n",
    "            log_probs = model(context_idxs)\n",
    "            val_loss = loss_function(log_probs.unsqueeze(0), target)\n",
    "            total_val_loss += val_loss.item()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Validation Loss: {total_val_loss/len(val_data):.4f}')\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64582fd",
   "metadata": {},
   "source": [
    "### Prediction Function\n",
    "\n",
    "This code cell defines a function `predict_next_words` to generate predictions for the next words given a starting text sequence. The function takes the trained model, starting text, word-to-index and index-to-word mappings, and the number of words to predict.\n",
    "\n",
    "**Functionality:**\n",
    "- The starting text is cleaned and lemmatized to obtain the last two words as context.\n",
    "- Using the model, the next words are predicted iteratively based on the context.\n",
    "- The predicted words are appended to a list and returned.\n",
    "\n",
    "**Arguments:**\n",
    "- `model`: Trained RNN model for word prediction.\n",
    "- `text`: Starting text sequence.\n",
    "- `word_to_ix`: Dictionary mapping words to indices.\n",
    "- `ix_to_word`: Dictionary mapping indices to words.\n",
    "- `num_words`: Number of words to predict.\n",
    "\n",
    "**Returns:**\n",
    "- `predicted_words`: List of predicted words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e4d7161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sarah', 'remember'] [77, 82]\n",
      "The predicted next five words are: ['ahead', 'moment', 'knew', 'venturing', 'side', 'rock', 'rhythmic', 'symphony', 'soothing', 'welcomed']\n",
      "\n",
      "sarah remember ahead moment knew venturing side rock rhythmic symphony soothing welcomed\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "def predict_next_words(model, text, word_to_ix, ix_to_word, num_words=5):\n",
    "    words = clean_text_Lemmatization(text)\n",
    "    context = [word_to_ix[words[-2]], word_to_ix[words[-1]]]\n",
    "    predicted_words = []\n",
    "    print(words, context)\n",
    "    for _ in range(num_words):\n",
    "        context_tensor = torch.tensor(context, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            output = model(context_tensor)\n",
    "        predicted_ix = torch.argmax(output).item()\n",
    "        predicted_word = ix_to_word[predicted_ix]\n",
    "        predicted_words.append(predicted_word)\n",
    "        context = [context[-1], predicted_ix]  # Update context for next prediction\n",
    "    return predicted_words\n",
    "\n",
    "text = 'sarah remember'\n",
    "predicted_words = predict_next_words(model, text, word_to_ix, ix_to_word, num_words=10)\n",
    "print(f'The predicted next five words are: {predicted_words}')\n",
    "print('\\n' + text + ' ' + ' '.join(predicted_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dbac49",
   "metadata": {},
   "source": [
    "### Prediction Function with Temperature Sampling\n",
    "\n",
    "This code cell defines a modified prediction function `predict_next_word_temperature` to generate predictions for the next words using temperature sampling. Temperature sampling is a technique used to control the randomness of predictions generated by the model.\n",
    "\n",
    "**Functionality:**\n",
    "- The starting text is cleaned and lemmatized to obtain the last two words as context.\n",
    "- Using the model, the next words are predicted iteratively based on the context and temperature sampling.\n",
    "- The predicted words are appended to a list and returned.\n",
    "\n",
    "**Arguments:**\n",
    "- `model`: Trained RNN model for word prediction.\n",
    "- `text`: Starting text sequence.\n",
    "- `word_to_ix`: Dictionary mapping words to indices.\n",
    "- `ix_to_word`: Dictionary mapping indices to words.\n",
    "- `num_words`: Number of words to predict.\n",
    "- `temperature`: Parameter controlling the randomness of predictions. Higher values result in more randomness.\n",
    "\n",
    "**Returns:**\n",
    "- `predicted_words`: List of predicted words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a21d5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted next five words are: ['ahead', 'mirroring', 'knew', 'charged', 'vastness', 'air', 'something', 'electric', 'charged', 'watched']\n",
      "\n",
      "sarah remember ahead mirroring knew charged vastness air something electric charged watched\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word_temperature(model, text, word_to_ix, ix_to_word, num_words=5, temperature=1.0):\n",
    "    words = clean_text_Lemmatization(text)\n",
    "    context = [word_to_ix.get(words[-2], 0), word_to_ix.get(words[-1], 0)]\n",
    "    predicted_words = []\n",
    "    for _ in range(num_words):\n",
    "        context_tensor = torch.tensor(context, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            output = model(context_tensor)\n",
    "        output = output / temperature\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=0)\n",
    "        predicted_ix = torch.multinomial(probabilities, 1).item()\n",
    "        predicted_word = ix_to_word[predicted_ix]\n",
    "        predicted_words.append(predicted_word)\n",
    "        context = [context[-1], predicted_ix]  # Update context for next prediction\n",
    "    return predicted_words\n",
    "\n",
    "text = 'sarah remember'\n",
    "predicted_words = predict_next_word_temperature(model, text, word_to_ix, ix_to_word, num_words=10, temperature=1)\n",
    "print(f'The predicted next five words are: {predicted_words}')\n",
    "print('\\n' + text + ' ' + ' '.join(predicted_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b8493",
   "metadata": {},
   "source": [
    "## Project Completed by:\n",
    "[Karim Nasr](https://www.linkedin.com/in/karim-nasr-abu-al-fath/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
